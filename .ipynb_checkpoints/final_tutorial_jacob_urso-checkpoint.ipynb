{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "The purpose of this program is to analyze sexual assault statistics in the College Park area. The data will be taken from 2010 to 2018 and will attempt to show the correlation between time of semester and sexual assault frequency.\n",
    "\n",
    "This page will walk through the steps taken to scrape, process, visualize, analyze, and utilize the data from the UMPD website.\n",
    "\n",
    "BY JACOB URSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection and Processing\n",
    "For this portion of the project you will need the following python packages:\n",
    "requests\n",
    "bs4\n",
    "pandas\n",
    "numpy\n",
    "json\n",
    "\n",
    "I have grouped together these two steps because they happen at roughly the same time. First I create an empty list that will be populated with dataframes for each month of data that is collected. I include the code snippet header=0 in the pandas function to ensure the column titles will not be read in as a datapoint. The next step is to tidy the data by fixing the location column. For some reason the location column was read in as a separate data point so I copy the location to the above data and then delete all of the nan rows. Lastly, reindex and display the head.\n",
    "\n",
    "DATA SOURCE:\n",
    "http://www.umpd.umd.edu/stats/incident_logs.cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "lst = []\n",
    "\n",
    "for year in range(2010, 2019):\n",
    "    for i in range(1, 13):\n",
    "        url = \"http://www.umpd.umd.edu/stats/incident_logs.cfm?year={}&month={}\"\n",
    "        r = requests.get(url.format(year, i))\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        df = pandas.read_html(str(soup.find(\"table\")), header = 0)\n",
    "        lst.append(df[0])\n",
    "full_df = pandas.concat(lst)\n",
    "full_df[\"LOCATION\"] = 'N/A'\n",
    "for i in full_df.index:\n",
    "    if i % 2 == 1:\n",
    "        full_df.at[i - 1, \"LOCATION\"] = full_df.at[i, \"UMPD CASENUMBER\"]\n",
    "full_df.dropna(inplace=True)\n",
    "full_df.index = range(len(full_df))\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to just pull out the data that is associated with sexual assault which required some analysis of the data. I noticed all the sexual assault cases had one of three keywords in the 'TYPE' column so I pulled out any data points with those keywords. Next, a csv copy is saved so the requests do not have to be made again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(full_df.values):\n",
    "    if 'Title IX' not in row[3] and 'Rape' not in row[3] and 'Stalking' not in row[3]:\n",
    "        full_df.drop(i, inplace=True)\n",
    "full_df.index = range(len(full_df))\n",
    "full_df.to_csv('full_title_nine_data.csv')\n",
    "full_df\n",
    "\n",
    "FOR MORE INFO ON THE PANDAS DROP FUNCTION:\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to further process the data for easier visualization. To do this the locations had to be trimmed down to necessary components while also being made more specific with the CP Maryland bit being added. From here, I then use the new location to create an HTTP get request to Google's geocode API which can return lat and long coordinates for any location. By altering the location variables earlier I have ensured that all the requests will return with a 200 status message and load the response using a json object into the proper columns of the data frame.\n",
    "\n",
    "PART OF THIS CODE SEGMENT REQUIRES A UNIQUE GOOGLE API KEY GO HERE TO GET ONE:\n",
    "https://developers.google.com/maps/documentation/geocoding/get-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(full_df.values):\n",
    "    if 'at' in row[5]:\n",
    "        words = row[5].split(' at ')\n",
    "        full_df.at[i, 'LOCATION'] = '{}, College Park, Maryland'.format(words[len(words) - 1]) \n",
    "    elif 'block of' in row[5]:\n",
    "        words = row[5].split(' block of ')\n",
    "        full_df.at[i, 'LOCATION'] = '{} {}, College Park, Maryland'.format(words[0], words[1])\n",
    "    else:\n",
    "        full_df.at[i, 'LOCATION'] = '{}, College Park, Maryland'.format(row[5])\n",
    "response = {}\n",
    "full_df['LAT'] = np.nan\n",
    "full_df['LNG'] = np.nan\n",
    "for i in full_df.index:\n",
    "    r = requests.get('https://maps.googleapis.com/maps/api/geocode/json?', params={'address':full_df.at[i, 'LOCATION'], 'key':'AIzaSyATvxT46YZdtBIqn1or9AzO-i-G0hgU3n8'})\n",
    "    response = json.loads(r.text)\n",
    "    full_df.at[i, 'LAT'] = response['results'][0]['geometry']['location']['lat']\n",
    "    full_df.at[i, 'LNG'] = response['results'][0]['geometry']['location']['lng']\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization\n",
    "For the next step the following python packages are required\n",
    "folium\n",
    "seaborn\n",
    "matplotlib\n",
    "\n",
    "In this step I create a heatmap to display the frequency of sexual assaults on campus and where the occur. All of this section is essentially just utilizing the folium package. The average lat and long are also calculated to place a marker on the map which will be important for future analysis.\n",
    "\n",
    "FOLIUM DOCUMENTION:\n",
    "http://python-visualization.github.io/folium/docs-v0.5.0/modules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "m = folium.Map(location=[38.985, -76.935], zoom_start=15)\n",
    "HeatMap(list(zip(full_df.LAT.values, full_df.LNG.values))).add_to(m)\n",
    "avg_lat = sum(full_df.LAT.values) / len(full_df.LAT.values)\n",
    "avg_lng = sum(full_df.LNG.values) / len(full_df.LNG.values)\n",
    "folium.Marker([avg_lat, avg_lng]).add_to(m)\n",
    "print(avg_lat)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to pull out the month and year from the data and create a bar graph to show the different frequencies of sexual assault based on time. This step was what directed the rest of my project as I noticed interesting patterns in relation to school semesters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['MONTH'] = np.nan\n",
    "full_df['YEAR'] = np.nan\n",
    "for i, row in enumerate(full_df.values):\n",
    "    current = row[1].split('/')\n",
    "    full_df.at[i, 'MONTH'] = int(current[0])\n",
    "    full_df.at[i, 'YEAR'] = 2000 + int(current[2].split()[0])\n",
    "print(full_df.hist(column='MONTH', bins = 23))\n",
    "print(full_df.hist(column='YEAR', bins = 19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are creating heatmaps for specific months and finding the average location and marking them. The code is essentially the same as above in terms of creating these heatmaps but I am setting the month equal to both 9 and 2 (september and february). This is because these are the months that mark the beginning of the semesters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[38.985, -76.935], zoom_start=15)\n",
    "october = full_df.loc[full_df['MONTH'] == 9]\n",
    "HeatMap(list(zip(october.LAT.values, october.LNG.values))).add_to(m)\n",
    "avg_lat = sum(october.LAT.values) / len(october.LAT.values)\n",
    "avg_lng = sum(october.LNG.values) / len(october.LNG.values)\n",
    "folium.Marker([avg_lat, avg_lng]).add_to(m)\n",
    "print(avg_lat)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[38.985, -76.935], zoom_start=15)\n",
    "first_semester = full_df.loc[full_df['MONTH'] == 2]\n",
    "HeatMap(list(zip(first_semester.LAT.values, first_semester.LNG.values))).add_to(m)\n",
    "avg_lat = sum(first_semester.LAT.values) / len(first_semester.LAT.values)\n",
    "avg_lng = sum(first_semester.LNG.values) / len(first_semester.LNG.values)\n",
    "folium.Marker([avg_lat, avg_lng]).add_to(m)\n",
    "print(avg_lat)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing I did was create a violin plot to show the distribution of sexual assaults based on north south location on campus. I did this by plotting latitude vs month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "plt.figure()\n",
    "seaborn.violinplot(x='MONTH', y='LAT', data=full_df.loc[full_df['LAT'] > 38.91])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways from visualization\n",
    "Heatmap(Total): This showed me that the assaults clustered in two main locations on campus. Specifically, the clusters were around freshman and sophomore living spaces.\n",
    "Bar Graphs: The bar graphs showed me two things, the first of which is that the months that marked the beginning of a semester had higher levels of sexual assault in comparison to the months surrounding them, and the second was that sexual assault has been increasing in frequency in recent years\n",
    "Heatmap(Month): These graphs showed me that in the months that I mentioned earlier that mark the beginning of the semester, sexual assault locations become even more concentrated in freshmen dorms as the average latitude shifts towards the freshmen communities\n",
    "Violin Plot: Lastly the violin plot confirmed the notion that sexual assault makes its way north on campus in the early months of the semester as seen in the lack of skew compared to other months.\n",
    "\n",
    "Analysis and ML\n",
    "For this portion of the project I will be looking at the impact that year has had on sexual assault. In other words I will be attempting to answer the questions:\n",
    "Is sexual assault frequency increasing or decreasing?\n",
    "At what rate?\n",
    "Can it be modeled?\n",
    "And to move into the last section... what can be done?\n",
    "\n",
    "The first step to analysis was to manipulate the data a little bit further to make the regressions easier to code. After, the arrays are then reshaped and a linear regression is done using sklearn to output a line of best fit onto the plot. \n",
    "\n",
    "MORE INFORMATION ON LINEAR REGRESSIONS AND BASICS OF ML:\n",
    "https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "reports_per_year = {2009:0, 2010:0, 2011:0, 2012:0, 2013:0, 2014:0, 2015:0, 2016:0, 2017:0, 2018:0}\n",
    "for i, row in enumerate(full_df.values):\n",
    "    reports_per_year[row[9]] = reports_per_year[row[9]] + 1\n",
    "plt.figure()\n",
    "plt.plot(reports_per_year.keys(), reports_per_year.values(), 'o')\n",
    "\n",
    "in_x = np.reshape(list(reports_per_year.keys()), (10, 1))\n",
    "in_y = np.reshape(list(reports_per_year.values()), (10, 1))\n",
    "fit = linear_model.LinearRegression().fit(in_x, in_y)\n",
    "predicted_y_values = fit.predict(in_x)\n",
    "plt.plot(in_x, predicted_y_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the R squared value for this data shows that the line of best fit is fitting at roughly 0.82, but this number can be increased by trying to determine a better fit. Also the coefficients and prediction for 2019 are shown so that the potential methods of prediction this regression provide can be exemplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.score(in_x, in_y))\n",
    "print(fit.coef_)\n",
    "print(fit.predict([[2019]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data I think that an ideal degree for a polynomial regression will be 3 as a quadratic equation does not appear like it would fit the data very well. This is decision was made based on very surface level analysis and the degree could be different depending on what the programmer thinks is appropriate. However I decided to create a function that would allow for the degree to be an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_w_degree(x_set, y_set, deg:int):\n",
    "    x2 = np.reshape(list(x_set), (len(x_set), 1))\n",
    "    y2 = np.reshape(list(y_set), (len(y_set), 1))\n",
    "    feat = PolynomialFeatures(degree=deg)\n",
    "    years = feat.fit_transform(x2)\n",
    "    fit = linear_model.LinearRegression().fit(years, y2)\n",
    "    return fit, years\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reports_per_year.keys(), reports_per_year.values(), 'o')\n",
    "\n",
    "in_x = np.reshape(list(reports_per_year.keys()), (10, 1))\n",
    "fit, in_pred = fit_w_degree(reports_per_year.keys(), reports_per_year.values(), 3)\n",
    "predicted_y_values = fit.predict(in_pred)\n",
    "plt.plot(in_x, predicted_y_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this fit looks like it is much more accurate and the degree could continue to be increased to try to gain more accuracy if needed. This fit will be a better predictor of future years and will also provide better insight into policy decisions. The final coding step is to again pull out all of the important numbers f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.score(in_pred, in_y))\n",
    "print(fit.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R squared score increased again meaning that the fit is more accurate than the previous one\n",
    "\n",
    "Insight and Policy Decisions\n",
    "\n",
    "Based on all of the insight gained throughout this program, it is advisable for the University to consider these points\n",
    "\n",
    "-Sexual Assault is happening more frequently in regions of campus with more underclassmen at the beginning of the semester\n",
    "-Sexual Assault numbers increased from the years 2010 to 2017 according to this data and recent policies have begun to alleviate these problems as shown by 2018 numbers\n",
    "\n",
    "Administration should consider arming incoming freshmen with more knowledge to try to deal with the first point. Perhaps more required courses related to consent assault should exist. The other takeaway to help continue the trend of dealing with this problem is to continue with programs that have been used in recent years and improve upon them.\n",
    "\n",
    "Unfortunately, the dataset lacked more details such as gender, age, and other personal information which would have made more digging possible. This program aimed to fully display available sexual assault statistics in the college park area and educate policy makers on how to better these statistics. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
